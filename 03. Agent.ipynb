{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd4510-c3c4-4f2d-b749-7d3db45c8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface bitsandbytes torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c92b67-b4b6-4927-994f-98f2630e8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0a813-58bf-4512-8020-fe717e34de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device \" + device)\n",
    "base_path = \"./\"\n",
    "pdf_json_dir = 'document_parses/pdf_json'\n",
    "pmc_json_dir = 'document_parses/pmc_json'\n",
    "#base_path = \"/content/drive/MyDrive/Projektmunka Smoking and COVID19\"\n",
    "os.chdir(base_path)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8cfe6b-40af-4e1e-b05f-6ba06e79a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path, dtype=str)\n",
    "\n",
    "# Define smoking-related keywords (expand as needed)\n",
    "smoking_keywords = [\n",
    "    \"smoking\", \"smoker\", \"smoke\", \"ecigarett\", \"cigarett\",  \"tobacco\", \"cigarette\", \"nicotine\",\n",
    "    \"vaping\", \"vape\", \"e-cigarette\", \"smoker\", \"cigar\", \"weed\", \"marijuana\"\n",
    "]\n",
    "\n",
    "# Filter papers where title/abstract contains smoking-related terms\n",
    "filtered_papers = metadata[\n",
    "    metadata[\"title\"].str.lower().str.contains('|'.join(smoking_keywords), na=False) |\n",
    "    metadata[\"abstract\"].str.lower().str.contains('|'.join(smoking_keywords), na=False)\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(filtered_papers)} smoking-related papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe837228-9543-463e-959f-fdd0eee4c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cord_uid', 'title', 'abstract', 'publish_time', 'source_x', 'authors', 'pdf_json_files', 'pmc_json_files']\n",
    "\n",
    "filtered_papers = filtered_papers[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b328a-a124-4025-85ca-6f2e0c200bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cae4ca-a6e0-42eb-bad5-f78ba78a300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body_text(json_path):\n",
    "    \"\"\"Extract and concatenate all 'text' fields from 'body_text' in a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return ' '.join(para['text'] for para in data.get('body_text', []))\n",
    "    except Exception as e:\n",
    "        # Optionally print or log the error\n",
    "        return None\n",
    "\n",
    "def get_full_text(row):\n",
    "    # Try PDF JSON first\n",
    "    if pd.notna(row['pdf_json_files']):\n",
    "        for json_path in row['pdf_json_files'].split('; '):\n",
    "            full_path = os.path.join(base_path, json_path.strip())\n",
    "            if os.path.exists(full_path):\n",
    "                return extract_body_text(full_path)\n",
    "    return None  # Return empty dict if no files found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f2164-07a5-4e1d-9a6c-5e8850ec7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Extracting full text sections\")\n",
    "filtered_papers['full_text'] = filtered_papers.progress_apply(get_full_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c116c-21ba-4709-9355-33191b0aa99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79a00b-82cc-4fe9-9ff5-dc98b7fbc030",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers = filtered_papers.dropna(subset=['title', 'abstract', 'full_text'])\n",
    "filtered_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcaf6c-7855-4eb3-ac1a-3db3721b2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_papers.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df821311-03a9-45e0-9280-efe4196b5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers['combined_text'] = (\n",
    "    filtered_papers['title'].fillna('') + '. ' +\n",
    "    filtered_papers['abstract'].fillna('') + '. ' +\n",
    "    filtered_papers['full_text'].fillna('')\n",
    ")\n",
    "\n",
    "# Basic statistics\n",
    "filtered_papers['text_length'] = filtered_papers['combined_text'].str.len()\n",
    "print(filtered_papers['text_length'].describe())\n",
    "\n",
    "# Example anomaly filter: drop if text is too short or too long\n",
    "min_length = 200   # adjust as needed\n",
    "max_length = 30000 # adjust as needed\n",
    "filtered_papers = filtered_papers[\n",
    "    (filtered_papers['text_length'] >= min_length) &\n",
    "    (filtered_papers['text_length'] <= max_length)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e89550-55e9-463e-9f01-d5117ce3ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cf1df-6bf6-481e-97fe-c687527d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove non-UTF8 and normalize unicode\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    # Remove HTML/XML tags\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    # Remove LaTeX (very basic)\n",
    "    text = re.sub(r\"\\$.*?\\$\", \" \", text)\n",
    "    # Remove references like [1], (1), etc.\n",
    "    text = re.sub(r\"\\[\\d+\\]|\\(\\d+\\)\", \" \", text)\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r\"[^\\x20-\\x7E]\", \" \", text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Lowercase for stopword removal\n",
    "    text = text.lower()\n",
    "    # # Remove stopwords\n",
    "    # words = text.split()\n",
    "    # words = [word for word in words if word not in stop_words]\n",
    "    # text = \" \".join(words)\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361c856-babd-47cf-ab54-ba03673eac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_embed=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_name_llm=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "chunk_size=200\n",
    "persist_dir=\"storage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ccc2b-052e-49f7-b048-440039ad9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size):\n",
    "    words = text.split(\" \")\n",
    "    return [\n",
    "        \" \".join(words[i:i + chunk_size])\n",
    "        for i in range(0, len(words), chunk_size)\n",
    "    ]\n",
    "\n",
    "def prepare_documents(df, chunk_size, text_column=\"combined_text\"):\n",
    "    print(\"Chunking documents...\")\n",
    "    chunks = []\n",
    "    for text in tqdm(df[text_column].dropna().values):\n",
    "        for chunk in chunk_text(text, chunk_size):\n",
    "            chunks.append(Document(text=chunk))\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def build_index(documents, model_name_embed, device, persist_dir):\n",
    "    print(\"Building vector index with CUDA embeddings...\")\n",
    "    Settings.llm = None\n",
    "    Settings.embed_model = HuggingFaceEmbedding(\n",
    "        model_name=model_name_embed, device=device\n",
    "    )\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, show_progress=True, insert_batch_size=len(documents)\n",
    "    )\n",
    "    print(\"Persisting index to disk...\")\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    print(f\"VectorStoreIndex saved to {persist_dir}.\")\n",
    "    return index\n",
    "\n",
    "def load_index(persist_dir):\n",
    "    print(f\"Loading index from {persist_dir}...\")\n",
    "    loaded_storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    index = load_index_from_storage(loaded_storage_context)\n",
    "    print(\"Index loaded.\")\n",
    "    return index\n",
    "\n",
    "def setup_llm(model_name_llm):\n",
    "    print(\"Setting up local LLM...\")\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=model_name_llm,\n",
    "        tokenizer_name=model_name_llm,\n",
    "        context_window=2048,\n",
    "        max_new_tokens=256,\n",
    "        device_map=\"cuda:0\",\n",
    "        generate_kwargs={\"temperature\": 0.95, \"do_sample\": True},\n",
    "    )\n",
    "    Settings.llm = llm\n",
    "\n",
    "def setup_chat_engine(index, system_prompt=None):\n",
    "    print(\"Setting up chat engine...\")\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are a medical chatbot, able to have normal interactions. \"\n",
    "            \"You only answer based on the Cord19 dataset.\"\n",
    "        )\n",
    "    chat_engine = index.as_chat_engine(\n",
    "        chat_mode=\"context\",\n",
    "        memory=ChatMemoryBuffer.from_defaults(token_limit=32000),\n",
    "        system_prompt=system_prompt,\n",
    "    )\n",
    "    return chat_engine\n",
    "\n",
    "def chat(chat_engine):\n",
    "    print(\"Chatbot is ready! Type your question or 'quit' to exit.\")\n",
    "    while True:\n",
    "        query = input(\"> \")\n",
    "        if query.lower() == \"quit\":\n",
    "            break\n",
    "        print(\"Agent: \", end=\"\", flush=True)\n",
    "        response = chat_engine.stream_chat(query)\n",
    "        for token in response.response_gen:\n",
    "            print(token, end=\"\", flush=True)\n",
    "        print()\n",
    "    chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814000f-eece-4666-99bb-8f8b1bce7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame (replace with your actual loading code)\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "# For demonstration, let's assume df is already loaded and cleaned\n",
    "\n",
    "\n",
    "# Step 1: Prepare documents (chunking)\n",
    "documents = prepare_documents(filtered_papers, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49b645-4f9c-41b3-90fd-9a99fcf09af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build and persist the vector index\n",
    "build_index(documents, model_name_embed, device, persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a863a3-8a75-4e79-a133-ee96bb318c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the index (optional, for a new session)\n",
    "index = load_index(persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb27c7-3412-4e6e-9ca3-658a46657420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Setup the LLM\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=model_name_llm,       # Nyelvi modell beállítása\n",
    "    tokenizer_name=model_name_llm,   # Nyelvi modell tokenizátorának beállítása\n",
    "    context_window=2048,                                          # Maximum token limit\n",
    "    max_new_tokens=256,                                           # Válasz maximális hossza\n",
    "    device_map=\"cuda:0\",                                          # GPU használata,\n",
    "    generate_kwargs={\"temperature\": 0.95, \"do_sample\": True},     # Ezek a paraméterek befolyásolják a modell válaszainak véletlenszerűségét és kreativitását.\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5246b-4346-49bc-99c3-abd0b8ba0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Setup the chat engine\n",
    "chat_engine = setup_chat_engine(index, system_prompt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c266e4-bca9-40ef-b3cf-50a613ab4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Start chatting\n",
    "chat(chat_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70243985-a293-4040-bedb-fa221661ca4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
