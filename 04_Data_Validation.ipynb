{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device \" + device)\n",
    "base_path = \"./\"\n",
    "pdf_json_dir = 'document_parses/pdf_json'\n",
    "pmc_json_dir = 'document_parses/pmc_json'\n",
    "#base_path = \"/content/drive/MyDrive/Projektmunka Smoking and COVID19\"\n",
    "os.chdir(base_path)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path, dtype=str)\n",
    "\n",
    "# Define smoking-related keywords (expand as needed)\n",
    "smoking_keywords = [\n",
    "    \"smoking\", \"smoker\", \"smoke\", \"ecigarett\", \"cigarett\",  \"tobacco\", \"cigarette\", \"nicotine\",\n",
    "    \"vaping\", \"vape\", \"e-cigarette\", \"smoker\", \"cigar\", \"weed\", \"marijuana\"\n",
    "]\n",
    "\n",
    "# Filter papers where title/abstract contains smoking-related terms\n",
    "filtered_papers = metadata[\n",
    "    metadata[\"title\"].str.lower().str.contains('|'.join(smoking_keywords), na=False) |\n",
    "    metadata[\"abstract\"].str.lower().str.contains('|'.join(smoking_keywords), na=False)\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(filtered_papers)} smoking-related papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cord_uid', 'title', 'abstract', 'publish_time', 'source_x', 'authors', 'pdf_json_files', 'pmc_json_files']\n",
    "\n",
    "filtered_papers = filtered_papers[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body_text(json_path):\n",
    "    \"\"\"Extract and concatenate all 'text' fields from 'body_text' in a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return ' '.join(para['text'] for para in data.get('body_text', []))\n",
    "    except Exception as e:\n",
    "        # Optionally print or log the error\n",
    "        return None\n",
    "\n",
    "def get_full_text(row):\n",
    "    # Try PDF JSON first\n",
    "    if pd.notna(row['pdf_json_files']):\n",
    "        for json_path in row['pdf_json_files'].split('; '):\n",
    "            full_path = os.path.join(base_path, json_path.strip())\n",
    "            if os.path.exists(full_path):\n",
    "                return extract_body_text(full_path)\n",
    "    return None  # Return empty dict if no files found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Extracting full text sections\")\n",
    "filtered_papers['full_text'] = filtered_papers.progress_apply(get_full_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers = filtered_papers.dropna(subset=['title', 'abstract', 'full_text'])\n",
    "filtered_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_papers.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers['combined_text'] = (\n",
    "    filtered_papers['title'].fillna('') + '. ' +\n",
    "    filtered_papers['abstract'].fillna('') + '. ' +\n",
    "    filtered_papers['full_text'].fillna('')\n",
    ")\n",
    "\n",
    "# Basic statistics\n",
    "filtered_papers['text_length'] = filtered_papers['combined_text'].str.len()\n",
    "print(filtered_papers['text_length'].describe())\n",
    "\n",
    "# Example anomaly filter: drop if text is too short or too long\n",
    "min_length = 200   # adjust as needed\n",
    "max_length = 30000 # adjust as needed\n",
    "filtered_papers = filtered_papers[\n",
    "    (filtered_papers['text_length'] >= min_length) &\n",
    "    (filtered_papers['text_length'] <= max_length)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy for Data validation stuff\n",
    "df= pd.DataFrame()\n",
    "df=filtered_papers.copy()\n",
    "df.rename(columns={'full_text': 'article_text'}, inplace=True)\n",
    "df.rename(columns={'combined_text': 'full_text'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Keyword Relevance\n",
    "\n",
    "### Check if documents actually discuss COVID + smoking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords\n",
    "covid_terms = [\"covid\", \"sars-cov-2\", \"coronavirus\"]\n",
    "smoking_terms = smoking_keywords\n",
    "\n",
    "# Filter rows containing at least 1 COVID + 1 smoking term\n",
    "def is_relevant(text):\n",
    "    if isinstance(text, str):\n",
    "        has_covid = any(term in text.lower() for term in covid_terms)\n",
    "        has_smoking = any(term in text.lower() for term in smoking_terms)\n",
    "        return has_covid and has_smoking\n",
    "    return False\n",
    "# Apply to abstract/body text\n",
    "df['is_relevant'] = df['full_text'].apply(is_relevant) \n",
    "print(f\"Relevant documents: {df['is_relevant'].sum()}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relevance\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['is_relevant'].value_counts().plot(kind='bar', color=['red', 'green'])\n",
    "plt.title(\"Relevance of Documents (COVID + Smoking)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')\n",
    "\n",
    "# Plot publications over time\n",
    "plt.figure(figsize=(20, 7))\n",
    "df['publish_time'].dt.year.value_counts().sort_index().plot(kind='line', marker='o')\n",
    "plt.title(\"Publications per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP-Based Validation (Topic Coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Extract top keywords for COVID+smoking docs\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "tfidf_matrix = tfidf.fit_transform(df[df['is_relevant']]['full_text'])\n",
    "top_keywords = pd.Series(tfidf.get_feature_names_out()).sample(10, random_state=42)\n",
    "\n",
    "print(\"Top Keywords in Relevant Docs:\")\n",
    "print(top_keywords.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the spaCy model\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define the query and compute its vector\n",
    "query = \"Impact of smoking on COVID-19 severity\"\n",
    "query_vec = nlp(query).vector.reshape(1, -1)\n",
    "\n",
    "# Function to validate semantic similarity\n",
    "def validate_semantic_similarity(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        doc_vec = doc.vector.reshape(1, -1)\n",
    "        return cosine_similarity(query_vec, doc_vec)[0][0]\n",
    "    return 0\n",
    "\n",
    "# Apply with progress bar\n",
    "print(\"Calculating semantic similarities...\")\n",
    "df['semantic_score'] = df['full_text'].progress_apply(validate_semantic_similarity)\n",
    "\n",
    "# Sort results with progress indication\n",
    "print(\"Sorting results...\")\n",
    "result = df[['title', 'semantic_score']].sort_values('semantic_score', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\nTop results:\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "output_folder = \"text_data_chunks\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Load your DataFrame (replace with your actual data)\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# Split data into 10 chunks\n",
    "num_files = 40\n",
    "chunks = np.array_split(df['full_text'].dropna(), num_files)\n",
    "\n",
    "# Save each chunk to a separate .txt file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    file_path = os.path.join(output_folder, f\"text_chunk_{i+1}.txt\")\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for text in chunk:\n",
    "            if isinstance(text, str) and text.strip():\n",
    "                f.write(text.strip() + '\\n\\n')  # Add double newline between entries\n",
    "                \n",
    "    print(f\"Saved {len(chunk)} entries to {file_path}\")\n",
    "\n",
    "print(f\"\\nAll files saved to '{output_folder}' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Terms in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize vectorizer (ignore stopwords and terms shorter than 2 chars)\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2, token_pattern=r'(?u)\\b[A-Za-z]{3,}\\b')\n",
    "X = vectorizer.fit_transform(df['full_text'].astype(str))\n",
    "\n",
    "# Sum counts for each term\n",
    "term_counts = X.sum(axis=0)\n",
    "term_freq = [(word, term_counts[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "term_freq_sorted = sorted(term_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top 10\n",
    "print(\"Top 10 Terms:\")\n",
    "for term, freq in term_freq_sorted[:10]:\n",
    "    print(f\"{term}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract data for plotting\n",
    "top_10_terms = term_freq_sorted\n",
    "terms = [term for term, freq in top_10_terms]\n",
    "frequencies = [freq for term, freq in top_10_terms]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(terms, frequencies, color='skyblue')\n",
    "plt.title(\"Top 10 Most Frequent Terms in COVID-19/Smoking Literature\")\n",
    "plt.xlabel(\"Terms\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
