{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65677de8-50d7-4dad-b538-9e51ac680eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface bitsandbytes torch spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c774c9-63dc-4075-88b6-d8393910f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Imports and Setup\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "from llama_index.core import Settings, Document, StorageContext, VectorStoreIndex, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f2c3fa-937e-426b-971c-74903f02b399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/anton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2. Configuration\n",
    "# =========================\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "base_path = \"./\"\n",
    "os.chdir(base_path)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "model_name_embed = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_name_llm = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "chunk_size = 200\n",
    "persist_dir = \"storage\"\n",
    "\n",
    "# Switch: Use vector DB or not\n",
    "USE_VECTOR_DB = True  # Set to False to run agent without vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6e2e9-d80b-438e-b7aa-b0cea1397238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Data Loading & Filtering\n",
    "# =========================\n",
    "\n",
    "metadata_path = \"metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path, dtype=str)\n",
    "\n",
    "smoking_keywords = [\n",
    "    \"smoking\", \"smoker\", \"smoke\", \"ecigarett\", \"cigarett\", \"tobacco\", \"cigarette\", \"nicotine\",\n",
    "    \"vaping\", \"vape\", \"e-cigarette\", \"cigar\", \"weed\", \"marijuana\"\n",
    "]\n",
    "covid_terms = [\"covid\", \"sars-cov-2\", \"coronavirus\"]\n",
    "\n",
    "filtered_papers = metadata[\n",
    "    metadata[\"title\"].str.lower().str.contains('|'.join(smoking_keywords), na=False) |\n",
    "    metadata[\"abstract\"].str.lower().str.contains('|'.join(smoking_keywords), na=False)\n",
    "].copy()\n",
    "\n",
    "columns_to_keep = ['cord_uid', 'title', 'abstract', 'publish_time', 'source_x', 'authors', 'pdf_json_files', 'pmc_json_files']\n",
    "filtered_papers = filtered_papers[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346d670-85a0-4e3c-b836-ee4013ca3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Text Extraction & Preprocessing\n",
    "# =========================\n",
    "\n",
    "def extract_body_text(json_path):\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return ' '.join(para['text'] for para in data.get('body_text', []))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_full_text(row):\n",
    "    if pd.notna(row['pdf_json_files']):\n",
    "        for json_path in row['pdf_json_files'].split('; '):\n",
    "            full_path = os.path.join(base_path, json_path.strip())\n",
    "            if os.path.exists(full_path):\n",
    "                return extract_body_text(full_path)\n",
    "    return None\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting full text sections\")\n",
    "filtered_papers['full_text'] = filtered_papers.progress_apply(get_full_text, axis=1)\n",
    "\n",
    "filtered_papers = filtered_papers.dropna(subset=['title', 'abstract', 'full_text'])\n",
    "\n",
    "filtered_papers['combined_text'] = (\n",
    "    filtered_papers['title'].fillna('') + '. ' +\n",
    "    filtered_papers['abstract'].fillna('') + '. ' +\n",
    "    filtered_papers['full_text'].fillna('')\n",
    ")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"\\$.*?\\$\", \" \", text)\n",
    "    text = re.sub(r\"\\[\\d+\\]|\\(\\d+\\)\", \" \", text)\n",
    "    text = re.sub(r\"[^\\x20-\\x7E]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "filtered_papers['combined_text'] = filtered_papers['combined_text'].apply(clean_text)\n",
    "\n",
    "# --- Outlier Detection (Text Length) ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(df['full_text'].str.len(), bins=50, color='skyblue')\n",
    "plt.title(\"Distribution of Document Lengths\")\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5b79b-be67-4bf1-b883-81839ec5f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers['text_length'] = filtered_papers['combined_text'].str.len()\n",
    "min_length = 200\n",
    "max_length = 30000\n",
    "filtered_papers = filtered_papers[\n",
    "    (filtered_papers['text_length'] >= min_length) &\n",
    "    (filtered_papers['text_length'] <= max_length)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5973b-84ce-4d2b-8f8b-d7000c745bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Professional Data Validation\n",
    "# =========================\n",
    "\n",
    "df = filtered_papers.copy()\n",
    "df.rename(columns={'full_text': 'article_text', 'combined_text': 'full_text'}, inplace=True)\n",
    "\n",
    "# --- Completeness & Consistency ---\n",
    "print(\"Checking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nChecking for duplicate titles:\")\n",
    "print(df['title'].duplicated().sum())\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['title'])\n",
    "\n",
    "# --- Relevance Validation ---\n",
    "def is_relevant(text):\n",
    "    if isinstance(text, str):\n",
    "        has_covid = any(term in text.lower() for term in covid_terms)\n",
    "        has_smoking = any(term in text.lower() for term in smoking_keywords)\n",
    "        return has_covid and has_smoking\n",
    "    return False\n",
    "\n",
    "df['is_relevant'] = df['full_text'].apply(is_relevant)\n",
    "print(f\"Relevant documents: {df['is_relevant'].sum()}/{len(df)}\")\n",
    "\n",
    "# --- Topic Modeling (LDA) ---\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df[df['is_relevant']]['full_text'])\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{idx+1}:\")\n",
    "    print([terms[i] for i in topic.argsort()[-10:][::-1]])\n",
    "\n",
    "# --- Semantic Similarity Validation ---\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "query = \"Impact of smoking on COVID-19 severity\"\n",
    "query_vec = nlp(query).vector.reshape(1, -1)\n",
    "\n",
    "def validate_semantic_similarity(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        doc_vec = doc.vector.reshape(1, -1)\n",
    "        return cosine_similarity(query_vec, doc_vec)[0][0]\n",
    "    return 0\n",
    "\n",
    "print(\"Calculating semantic similarities...\")\n",
    "df['semantic_score'] = df['full_text'].progress_apply(validate_semantic_similarity)\n",
    "print(df[['title', 'semantic_score']].sort_values('semantic_score', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2461e7a2-f4ce-4293-86a4-417c5d14bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Agent Preparation\n",
    "# =========================\n",
    "\n",
    "def chunk_text(text, chunk_size):\n",
    "    words = text.split(\" \")\n",
    "    return [\n",
    "        \" \".join(words[i:i + chunk_size])\n",
    "        for i in range(0, len(words), chunk_size)\n",
    "    ]\n",
    "\n",
    "def prepare_documents(df, chunk_size, text_column=\"full_text\"):\n",
    "    print(\"Chunking documents...\")\n",
    "    chunks = []\n",
    "    for text in tqdm(df[text_column].dropna().values):\n",
    "        for chunk in chunk_text(text, chunk_size):\n",
    "            chunks.append(Document(text=chunk))\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def build_index(documents, model_name_embed, device, persist_dir):\n",
    "    print(\"Building vector index with CUDA embeddings...\")\n",
    "    Settings.llm = None\n",
    "    Settings.embed_model = HuggingFaceEmbedding(\n",
    "        model_name=model_name_embed, device=device\n",
    "    )\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, show_progress=True, insert_batch_size=len(documents)\n",
    "    )\n",
    "    print(\"Persisting index to disk...\")\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    print(f\"VectorStoreIndex saved to {persist_dir}.\")\n",
    "    return index\n",
    "\n",
    "def load_index(persist_dir):\n",
    "    print(f\"Loading index from {persist_dir}...\")\n",
    "    Settings.embed_model = HuggingFaceEmbedding(\n",
    "        model_name=model_name_embed, device=device\n",
    "    )\n",
    "    loaded_storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    index = load_index_from_storage(loaded_storage_context)\n",
    "    print(\"Index loaded.\")\n",
    "    return index\n",
    "\n",
    "def setup_llm(model_name_llm):\n",
    "    print(\"Setting up local LLM...\")\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=model_name_llm,\n",
    "        tokenizer_name=model_name_llm,\n",
    "        context_window=2048,\n",
    "        max_new_tokens=256,\n",
    "        device_map=\"cuda:0\",\n",
    "        generate_kwargs={\"temperature\": 0.95, \"do_sample\": True},\n",
    "    )\n",
    "    Settings.llm = llm\n",
    "\n",
    "def setup_chat_engine(index, system_prompt=None):\n",
    "    print(\"Setting up chat engine...\")\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are a medical chatbot, able to have normal interactions. \"\n",
    "            \"You only answer based on the provided context.\"\n",
    "        )\n",
    "    chat_engine = index.as_chat_engine(\n",
    "        chat_mode=\"context\",\n",
    "        memory=ChatMemoryBuffer.from_defaults(token_limit=32000),\n",
    "        system_prompt=system_prompt,\n",
    "    )\n",
    "    return chat_engine\n",
    "\n",
    "def chat(chat_engine):\n",
    "    print(\"Chatbot is ready! Type your question or 'quit' to exit.\")\n",
    "    llm = Settings.llm\n",
    "    if hasattr(llm, \"model_name\"):\n",
    "        print(\"Current LLM model:\", llm.model_name)\n",
    "    else:\n",
    "        print(\"Current LLM:\", type(llm))\n",
    "\n",
    "    while True:\n",
    "        query = input(\"> \")\n",
    "        if query.lower() == \"quit\":\n",
    "            break\n",
    "        print(\"Agent: \", end=\"\", flush=True)\n",
    "        response = chat_engine.stream_chat(query)\n",
    "        for token in response.response_gen:\n",
    "            print(token, end=\"\", flush=True)\n",
    "        print()\n",
    "        chat_engine.reset()\n",
    "\n",
    "def agent_with_vector_db(index, llm, system_prompt=None, top_k=5):\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are a medical chatbot. You only answer based on the provided context.\"\n",
    "        )\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "    print(\"Agent running in vector DB mode. Type your question or 'quit' to exit.\")\n",
    "    while True:\n",
    "        query = input(\"> \")\n",
    "        if query.lower() == \"quit\":\n",
    "            break\n",
    "        # 1. Retrieve relevant context\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        retrieved_texts = [node.get_content() for node in retrieved_nodes]\n",
    "        print(\"Context:\\n\", retrieved_texts)\n",
    "        # 2. Build prompt\n",
    "        context = \"\\n\\n\".join(retrieved_texts)\n",
    "        prompt = f\"{system_prompt}\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "        # 3. Get answer from LLM\n",
    "        response = llm.complete(prompt)\n",
    "        print(\"Agent:\", response)\n",
    "\n",
    "def index_exists(persist_dir):\n",
    "    return os.path.exists(persist_dir) and len(os.listdir(persist_dir)) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb033dc1-7a7e-498e-b734-c9e56cb6547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index found in 'storage'. Loading index...\n",
      "Loading index from storage...\n",
      "Index loaded.\n",
      "Setting up local LLM...\n",
      "Setting up chat engine...\n",
      "Chatbot is ready! Type your question or 'quit' to exit.\n",
      "Current LLM model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  What is the capital of France?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Yes, I understand. The capital of France is Paris.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Does smoking increase the risk of hospitalization for COVID-19 patients?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Yes, smoking can increase the risk of hospitalization for COVID-19 patients. A systematic review and meta-analysis of hospitalized COVID-19 patients found an average odds ratio of 1.9 for hospitalization for COVID-19 among smokers compared to non-smokers (95% confidence interval [CI]: 1.1-3.5). This means for every 100 people with COVID-19 who smokes, 19 people will experience hospitalization compared to 14 people who do not smoke. The overall hospitalization rate for COVID-19 was 9.2% in the US hospital system, and smoking was associated with an even higher risk of hospitalization, with 13% of hospitalized COVID-19 patients smoking compared to 1% of non-smokers. This suggests that smoking increases the risk of severe COVID-19 illness in hospitalized patients.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7. Agent Execution\n",
    "# =========================\n",
    "\n",
    "if USE_VECTOR_DB:\n",
    "    # --- With Vector Database ---\n",
    "    if index_exists(persist_dir):\n",
    "        print(f\"Index found in '{persist_dir}'. Loading index...\")\n",
    "        index = load_index(persist_dir)\n",
    "    else:\n",
    "        print(f\"No index found in '{persist_dir}'. Building new index...\")\n",
    "        documents = prepare_documents(df[df['is_relevant']], chunk_size)\n",
    "        build_index(documents, model_name_embed, device, persist_dir)\n",
    "        index = load_index(persist_dir)\n",
    "    setup_llm(model_name_llm)\n",
    "    chat_engine = setup_chat_engine(index)\n",
    "    chat(chat_engine)\n",
    "else:\n",
    "    # --- Without Vector Database: Simple Keyword Search + LLM ---\n",
    "    setup_llm(model_name_llm)\n",
    "    llm = Settings.llm\n",
    "    index = load_index(persist_dir)  # or build_index(...) if not already built\n",
    "    agent_with_vector_db(index, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b062eb8-cd0f-4535-88c5-1e8a7f1e9459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
