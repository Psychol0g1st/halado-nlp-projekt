{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88d856a-8bce-4ae9-9eef-a88ab5c38612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/anton/jupyter-env/lib/python3.12/site-packages (1.10.0)\n",
      "Requirement already satisfied: transformers in /home/anton/jupyter-env/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: nltk in /home/anton/jupyter-env/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: sentence-transformers in /home/anton/jupyter-env/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /home/anton/jupyter-env/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/anton/jupyter-env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /home/anton/jupyter-env/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/anton/jupyter-env/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/anton/jupyter-env/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/anton/jupyter-env/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /home/anton/jupyter-env/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anton/jupyter-env/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anton/jupyter-env/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anton/jupyter-env/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anton/jupyter-env/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/anton/jupyter-env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu transformers nltk sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68fdab8-217e-40b9-8ee3-387f8c756c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/anton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 11354/11354 [00:01<00:00, 6194.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 11354/11354 [00:04<00:00, 2437.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "tqdm.pandas()\n",
    "\n",
    "base_path = \"./\"\n",
    "os.chdir(base_path)\n",
    "\n",
    "# Load curated data\n",
    "df = pd.read_csv(\"smoking_covid_curated.csv\")\n",
    "\n",
    "# Clean text function\n",
    "stop_words = set(stopwords.words('english'))  # Define once for speed\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))  # Remove special chars\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to abstract\n",
    "df['clean_abstract'] = df['abstract'].progress_apply(clean_text)\n",
    "\n",
    "# Apply cleaning to full_text (JSON/dict-like field)\n",
    "def process_full_text(x):\n",
    "    if pd.isna(x) or x == {}:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            x = ast.literal_eval(x)  # Convert string to dict\n",
    "        if not isinstance(x, dict):\n",
    "            return \"\"\n",
    "        return ' '.join(clean_text(t) for section in x.values() for t in section)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "df['clean_full_text'] = df['full_text'].progress_apply(process_full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093dcd3a-41a5-4b93-b364-20336d9d7567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11354/11354 [00:00<00:00, 70319.61it/s]\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df.progress_apply(\n",
    "    lambda x: ' '.join(part for part in [str(x.get('title', '')), x.get('clean_abstract', ''), x.get('clean_full_text', '')] if part),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ab5031-ab10-4faf-a3fb-1785f7b0a9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>source_x</th>\n",
       "      <th>authors</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>pmc_json_files</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>clean_full_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8qnrcgnk</td>\n",
       "      <td>Heme oxygenase-1 and carbon monoxide in pulmon...</td>\n",
       "      <td>Heme oxygenase-1 (HO-1), an inducible stress p...</td>\n",
       "      <td>2003-08-07</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Slebos, Dirk-Jan; Ryter, Stefan W; Choi, Augus...</td>\n",
       "      <td>document_parses/pdf_json/faaf1022ccfe93b032c56...</td>\n",
       "      <td>document_parses/pmc_json/PMC193681.xml.json</td>\n",
       "      <td>{'introduction': ['The heme oxygenase-1/carbon...</td>\n",
       "      <td>heme oxygenase ho inducible stress protein con...</td>\n",
       "      <td>heme oxygenasecarbon monoxide hoco system rece...</td>\n",
       "      <td>Heme oxygenase-1 and carbon monoxide in pulmon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qva0jt86</td>\n",
       "      <td>Relevance of human metapneumovirus in exacerba...</td>\n",
       "      <td>BACKGROUND AND METHODS: Human metapneumovirus ...</td>\n",
       "      <td>2005-12-21</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Rohde, G; Borg, I; Arinir, U; Kronsbein, J; Ra...</td>\n",
       "      <td>document_parses/pdf_json/4ba79e54ecf81b30b5646...</td>\n",
       "      <td>document_parses/pmc_json/PMC1334186.xml.json</td>\n",
       "      <td>{'methods': ['Three different groups were stud...</td>\n",
       "      <td>background methods human metapneumovirus hmpv ...</td>\n",
       "      <td>three different groups studied first group con...</td>\n",
       "      <td>Relevance of human metapneumovirus in exacerba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bnnl700a</td>\n",
       "      <td>Public awareness of risk factors for cancer am...</td>\n",
       "      <td>BACKGROUND: The present study aimed to provide...</td>\n",
       "      <td>2006-01-10</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Inoue, Manami; Iwasaki, Motoki; Otani, Tetsuya...</td>\n",
       "      <td>document_parses/pdf_json/a78fd1b34372e1e54bf2a...</td>\n",
       "      <td>document_parses/pmc_json/PMC1351169.xml.json</td>\n",
       "      <td>{'methods': ['The study was conducted as a par...</td>\n",
       "      <td>background present study aimed provide informa...</td>\n",
       "      <td>study conducted part omnibus survey december c...</td>\n",
       "      <td>Public awareness of risk factors for cancer am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ft5wl70x</td>\n",
       "      <td>Involvement of microRNAs in physiological and ...</td>\n",
       "      <td>To date, at least 900 different microRNA (miRN...</td>\n",
       "      <td>2010-11-23</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Tomankova, Tereza; Petrek, Martin; Kriegova, Eva</td>\n",
       "      <td>document_parses/pdf_json/b97de55ba907c3b1f3048...</td>\n",
       "      <td>document_parses/pmc_json/PMC3001429.xml.json</td>\n",
       "      <td>{'references': []}</td>\n",
       "      <td>date least different microrna mirna genes disc...</td>\n",
       "      <td></td>\n",
       "      <td>Involvement of microRNAs in physiological and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1h6jz1h5</td>\n",
       "      <td>Plant Plastid Engineering</td>\n",
       "      <td>Genetic material in plants is distributed into...</td>\n",
       "      <td>2010-11-03</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Wani, Shabir H.; Haider, Nadia; Kumar, Hitesh;...</td>\n",
       "      <td>document_parses/pdf_json/79979652a864cef3a4134...</td>\n",
       "      <td>document_parses/pmc_json/PMC3048312.xml.json</td>\n",
       "      <td>{'introduction': [\"Genetic material in plants ...</td>\n",
       "      <td>genetic material plants distributed nucleus pl...</td>\n",
       "      <td>genetic material plants distributed nucleus ch...</td>\n",
       "      <td>Plant Plastid Engineering genetic material pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                              title  \\\n",
       "0  8qnrcgnk  Heme oxygenase-1 and carbon monoxide in pulmon...   \n",
       "1  qva0jt86  Relevance of human metapneumovirus in exacerba...   \n",
       "2  bnnl700a  Public awareness of risk factors for cancer am...   \n",
       "3  ft5wl70x  Involvement of microRNAs in physiological and ...   \n",
       "4  1h6jz1h5                          Plant Plastid Engineering   \n",
       "\n",
       "                                            abstract publish_time source_x  \\\n",
       "0  Heme oxygenase-1 (HO-1), an inducible stress p...   2003-08-07      PMC   \n",
       "1  BACKGROUND AND METHODS: Human metapneumovirus ...   2005-12-21      PMC   \n",
       "2  BACKGROUND: The present study aimed to provide...   2006-01-10      PMC   \n",
       "3  To date, at least 900 different microRNA (miRN...   2010-11-23      PMC   \n",
       "4  Genetic material in plants is distributed into...   2010-11-03      PMC   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Slebos, Dirk-Jan; Ryter, Stefan W; Choi, Augus...   \n",
       "1  Rohde, G; Borg, I; Arinir, U; Kronsbein, J; Ra...   \n",
       "2  Inoue, Manami; Iwasaki, Motoki; Otani, Tetsuya...   \n",
       "3   Tomankova, Tereza; Petrek, Martin; Kriegova, Eva   \n",
       "4  Wani, Shabir H.; Haider, Nadia; Kumar, Hitesh;...   \n",
       "\n",
       "                                      pdf_json_files  \\\n",
       "0  document_parses/pdf_json/faaf1022ccfe93b032c56...   \n",
       "1  document_parses/pdf_json/4ba79e54ecf81b30b5646...   \n",
       "2  document_parses/pdf_json/a78fd1b34372e1e54bf2a...   \n",
       "3  document_parses/pdf_json/b97de55ba907c3b1f3048...   \n",
       "4  document_parses/pdf_json/79979652a864cef3a4134...   \n",
       "\n",
       "                                 pmc_json_files  \\\n",
       "0   document_parses/pmc_json/PMC193681.xml.json   \n",
       "1  document_parses/pmc_json/PMC1334186.xml.json   \n",
       "2  document_parses/pmc_json/PMC1351169.xml.json   \n",
       "3  document_parses/pmc_json/PMC3001429.xml.json   \n",
       "4  document_parses/pmc_json/PMC3048312.xml.json   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  {'introduction': ['The heme oxygenase-1/carbon...   \n",
       "1  {'methods': ['Three different groups were stud...   \n",
       "2  {'methods': ['The study was conducted as a par...   \n",
       "3                                 {'references': []}   \n",
       "4  {'introduction': [\"Genetic material in plants ...   \n",
       "\n",
       "                                      clean_abstract  \\\n",
       "0  heme oxygenase ho inducible stress protein con...   \n",
       "1  background methods human metapneumovirus hmpv ...   \n",
       "2  background present study aimed provide informa...   \n",
       "3  date least different microrna mirna genes disc...   \n",
       "4  genetic material plants distributed nucleus pl...   \n",
       "\n",
       "                                     clean_full_text  \\\n",
       "0  heme oxygenasecarbon monoxide hoco system rece...   \n",
       "1  three different groups studied first group con...   \n",
       "2  study conducted part omnibus survey december c...   \n",
       "3                                                      \n",
       "4  genetic material plants distributed nucleus ch...   \n",
       "\n",
       "                                                text  \n",
       "0  Heme oxygenase-1 and carbon monoxide in pulmon...  \n",
       "1  Relevance of human metapneumovirus in exacerba...  \n",
       "2  Public awareness of risk factors for cancer am...  \n",
       "3  Involvement of microRNAs in physiological and ...  \n",
       "4  Plant Plastid Engineering genetic material pla...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791c7f43-50f7-4ce7-b6d9-51d181a092fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 11354/11354 [02:07<00:00, 88.83it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "df['embedding'] = df['text'].progress_apply(lambda x: embedding_model.encode(x, show_progress_bar=False))\n",
    "normalized_embeddings = normalize(np.vstack(df['embedding'].values))\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatIP(384)\n",
    "index.add(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d0aacb3-70d8-4197-8a2d-d6db7d228c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff405fb90ce5493096b89f6c86d45137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2a942dfbd145ccad0e3ac565ae85d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbfe77821e94ea89e767dca6084aeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a668e39c04b14eefbae01b1cb16b8753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7429c99a3b5f48b184d94b738764eacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a1ccb2085f483ea8ece78650bb3b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddabbd3d9e241db83764b60584ec8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/656M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/biomed_roberta_base\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"allenai/biomed_roberta_base\")\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0edc675b-757d-4170-8f79-1ffc016ce998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=400, overlap=50):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        n_tokens = len(sentence.split())\n",
    "        if tokens + n_tokens > max_tokens:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = chunk[-overlap:]  # Retain overlap\n",
    "            tokens = sum(len(c.split()) for c in chunk)\n",
    "        chunk.append(sentence)\n",
    "        tokens += n_tokens\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_answer(question, context_chunk):\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=context_chunk, truncation=True)\n",
    "        return result['answer'], result['score']\n",
    "    except Exception as e:\n",
    "        return \"No answer found\", 0.0\n",
    "\n",
    "def answer_question(question, top_k=5, alpha=0.5):\n",
    "    # Step 1: Retrieve top documents\n",
    "    question_embedding = normalize(embedding_model.encode([question]))\n",
    "    D, I = index.search(question_embedding, top_k)\n",
    "\n",
    "    answers = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        paper = df.iloc[idx]\n",
    "        chunks = chunk_text(paper['text'])\n",
    "\n",
    "        for chunk in chunks:\n",
    "            answer, score = extract_answer(question, chunk)\n",
    "            if answer and score > 0:\n",
    "                sim = D[0][i]  # cosine similarity\n",
    "                confidence = alpha * sim + (1 - alpha) * score\n",
    "                answers.append({\n",
    "                    \"source\": paper['title'],\n",
    "                    \"answer\": answer,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"qa_score\": score,\n",
    "                    \"semantic_similarity\": sim,\n",
    "                    \"context\": chunk\n",
    "                })\n",
    "\n",
    "    return sorted(answers, key=lambda x: x[\"confidence\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "393eec48-ff62-490d-bc1e-a1078a8d2556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mQuestion:\u001b[0m What is COVID-19?\n",
      "\n",
      "\u001b[1mAnswer 1:\u001b[0m 19\n",
      "\u001b[1mSource:\u001b[0m COVID-19 and smoking\n",
      "\u001b[1mConfidence:\u001b[0m 0.36\n",
      "\u001b[1mContext:\u001b[0m [...]COVID-19 and smoking[...]\n",
      "\n",
      "\n",
      "\u001b[1mAnswer 2:\u001b[0m 19\n",
      "\u001b[1mSource:\u001b[0m COVID-19 and Smoking\n",
      "\u001b[1mConfidence:\u001b[0m 0.36\n",
      "\u001b[1mContext:\u001b[0m [...]COVID-19 and Smoking[...]\n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m What is the effect of nicotine on ACE2 receptors?\n",
      "\n",
      "\u001b[1mAnswer 1:\u001b[0m : Possible Relevance\n",
      "\u001b[1mSource:\u001b[0m Late Breaking Abstract-ACE2 Overexpression Modulates Nicotine Receptors In Cell Type Specific Manner: Possible Relevance In Covid-19\n",
      "\u001b[1mConfidence:\u001b[0m 0.38\n",
      "\u001b[1mContext:\u001b[0m [...]Late Breaking Abstract-ACE2 Overexpression Modulates Nicotine Receptors In Cell Type Specific Manner: Possible Relevance In Covid-19[...]\n",
      "\n",
      "\n",
      "\u001b[1mAnswer 2:\u001b[0m nicotine as a mediator\n",
      "\u001b[1mSource:\u001b[0m COVID-19 and nicotine as a mediator of ACE-2\n",
      "\u001b[1mConfidence:\u001b[0m 0.36\n",
      "\u001b[1mContext:\u001b[0m [...]COVID-19 and nicotine as a mediator of ACE-2[...]\n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m How does vaping affect lung inflammation in coronavirus cases?\n",
      "\n",
      "\u001b[1mAnswer 1:\u001b[0m injury\n",
      "\u001b[1mSource:\u001b[0m What are the mechanisms underlying vaping-induced lung injury?\n",
      "\u001b[1mConfidence:\u001b[0m 0.37\n",
      "\u001b[1mContext:\u001b[0m [...]What are the mechanisms underlying vaping-induced lung injury?[...]\n",
      "\n",
      "\n",
      "\u001b[1mAnswer 2:\u001b[0m injury\n",
      "\u001b[1mSource:\u001b[0m What are the mechanisms underlying vaping-induced lung injury?\n",
      "\u001b[1mConfidence:\u001b[0m 0.37\n",
      "\u001b[1mContext:\u001b[0m [...]What are the mechanisms underlying vaping-induced lung injury?[...]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41579991612c4e38beacaaab6abdbdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/656M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is COVID-19?\",\n",
    "    \"What is the effect of nicotine on ACE2 receptors?\",\n",
    "    \"How does vaping affect lung inflammation in coronavirus cases?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n\\033[1mQuestion:\\033[0m {q}\")\n",
    "    results = answer_question(q)\n",
    "    for i, r in enumerate(results[:2]):\n",
    "        print(f\"\\n\\033[1mAnswer {i+1}:\\033[0m {r['answer']}\")\n",
    "        print(f\"\\033[1mSource:\\033[0m {r['source']}\")\n",
    "        print(f\"\\033[1mConfidence:\\033[0m {r['confidence']:.2f}\")\n",
    "        print(f\"\\033[1mContext:\\033[0m [...]{r['context'][:300]}[...]\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
